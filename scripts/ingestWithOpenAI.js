// scripts/ingestWithOpenAI.js
import fs from "fs";
import OpenAI from "openai";
import { MongoClient } from "mongodb";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const client = new MongoClient(process.env.MONGODB_URI);
const dbName = "rareearthminerals";

async function ingestPDF(filePath) {
  console.log(`ðŸ“„ Uploading ${filePath} to OpenAI...`);

  // 1ï¸âƒ£ Upload the raw PDF
  const upload = await openai.files.create({
    file: fs.createReadStream(filePath),
    purpose: "assistants",
  });

  console.log("ðŸ§  Extracting structured text...");
  // 2ï¸âƒ£ Extract clean, structured text
  const extraction = await openai.responses.create({
    model: "gpt-4.1-mini",
    input: [
      {
        role: "user",
        content: [
          {
            type: "input_text",
            text:
              "Extract and structure the text from this EU Regulation PDF. " +
              "Preserve all article numbers, annex titles, and section headers. " +
              "Return a continuous text format suitable for vector embedding.",
          },
          { type: "input_file", file_id: upload.id },
        ],
      },
    ],
  });

  const text = extraction.output_text?.trim() || "";
  if (!text) throw new Error("No text returned from OpenAI.");

  console.log(`ðŸª¶ Extracted ${text.length.toLocaleString()} characters`);

  // 3ï¸âƒ£ Chunk intelligently by regulation sections
  console.log("ðŸ” Splitting into sections...");
  const sections = text
    .split(/(?=Article\s+\d+|Annex\s+[IVXLC]+|Chapter\s+\d+)/i)
    .map((chunk) => chunk.trim())
    .filter((chunk) => chunk.length > 100);

  console.log(`ðŸ§© ${sections.length} sections detected`);

  // 4ï¸âƒ£ Create embeddings + metadata per section
  await client.connect();
  const db = client.db(dbName);
  const collection = db.collection("documents");

  for (const section of sections) {
    console.log("ðŸ§¬ Embedding section...");
    const emb = await openai.embeddings.create({
      model: "text-embedding-3-large",
      input: section.slice(0, 8000),
    });

    // 5ï¸âƒ£ Semantic tagging: identify risk domain
    const tagsResp = await openai.responses.create({
      model: "gpt-4.1-mini",
      input: [
        {
          role: "user",
          content: [
            {
              type: "input_text",
              text:
                "From this section of EU regulation text, identify the key domain tags (e.g., 'Battery Passport', 'Supply Chain', 'Compliance', 'ESG', 'Reporting', 'Thermal Risk'). " +
                "Return a JSON array of 2â€“5 tags only.",
            },
            { type: "input_text", text: section.slice(0, 2000) },
          ],
        },
      ],
    });

    let tags = [];
    try {
      tags = JSON.parse(tagsResp.output_text);
    } catch {
      tags = ["General Regulation"];
    }

    // 6ï¸âƒ£ Save section
    await collection.insertOne({
      text: section,
      embedding: emb.data[0].embedding,
      metadata: {
        source: filePath.split("/").pop(),
        sectionTitle: section.slice(0, 80),
        tags,
        type: "EU Regulation",
        date: new Date().toISOString(),
      },
    });
    console.log(`âœ… Section stored with tags: ${tags.join(", ")}`);
  }

  console.log("ðŸŽ¯ Ingestion complete â€” all sections embedded and tagged");
  await client.close();
}

// --- CLI entrypoint ---
const filePath = process.argv[2];
if (!filePath) {
  console.error("âŒ Usage: node scripts/ingestWithOpenAI.js ./public/filename.pdf");
  process.exit(1);
}

ingestPDF(filePath).catch((err) => {
  console.error("ðŸ’¥ Error during ingestion:", err);
  process.exit(1);
});

